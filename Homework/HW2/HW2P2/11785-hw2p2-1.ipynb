{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbXkUQWFLBRF"
   },
   "source": [
    "# HW2P2: Image Recognition and Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "695K5zs36a48"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:48:11.368074Z",
     "iopub.status.busy": "2025-02-27T05:48:11.367777Z",
     "iopub.status.idle": "2025-02-27T05:48:11.545052Z",
     "shell.execute_reply": "2025-02-27T05:48:11.543912Z",
     "shell.execute_reply.started": "2025-02-27T05:48:11.368042Z"
    },
    "id": "sQr0ss8w6jVI",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi # Run this to see what GPU you have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:48:33.757601Z",
     "iopub.status.busy": "2025-02-27T05:48:33.757227Z",
     "iopub.status.idle": "2025-02-27T05:48:42.390354Z",
     "shell.execute_reply": "2025-02-27T05:48:42.389156Z",
     "shell.execute_reply.started": "2025-02-27T05:48:33.757570Z"
    },
    "id": "MmbTatic6PDX",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install wandb --quiet # Install WandB\n",
    "!pip install pytorch_metric_learning --quiet #Install the Pytorch Metric Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:48:51.216138Z",
     "iopub.status.busy": "2025-02-27T05:48:51.215765Z",
     "iopub.status.idle": "2025-02-27T05:48:59.387817Z",
     "shell.execute_reply": "2025-02-27T05:48:59.386970Z",
     "shell.execute_reply.started": "2025-02-27T05:48:51.216112Z"
    },
    "id": "_oCzGBTh6xjL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.v2 as T\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics as mt\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import glob\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_metric_learning import samplers\n",
    "import csv\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-26T23:09:03.814Z"
    },
    "id": "MT9MZk9p69Q5",
    "outputId": "5187e528-3105-45b7-fd38-2643140e468d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive # Link to your drive if you are not using Colab with GCP\n",
    "# drive.mount('/content/drive') # Models in this HW take a long time to get trained and make sure to save it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gf6Da1K37BSJ"
   },
   "source": [
    "# Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-26T23:09:03.814Z"
    },
    "id": "Z1Uu5z2K7AS3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO: Use the same Kaggle code from HW1P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:49:36.557857Z",
     "iopub.status.busy": "2025-02-27T05:49:36.557532Z",
     "iopub.status.idle": "2025-02-27T05:49:36.695386Z",
     "shell.execute_reply": "2025-02-27T05:49:36.694471Z",
     "shell.execute_reply.started": "2025-02-27T05:49:36.557833Z"
    },
    "id": "sNRYbTmU7Dk3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Reminder: Make sure you have connected your kaggle API before running this block\n",
    "# !mkdir '/content/data'\n",
    "\n",
    "# !kaggle competitions download -c 11785-hw-2-p-2-face-verification-spring-2025\n",
    "# !unzip -qo '11785-hw-2-p-2-face-verification-spring-2025' -d '/content/data'\n",
    "\n",
    "!ls /kaggle/input/11785-hw-2-p-2-face-verification-spring-2025/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OgkfYwP7HVt"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:49:38.057815Z",
     "iopub.status.busy": "2025-02-27T05:49:38.057387Z",
     "iopub.status.idle": "2025-02-27T05:49:38.063390Z",
     "shell.execute_reply": "2025-02-27T05:49:38.062555Z",
     "shell.execute_reply.started": "2025-02-27T05:49:38.057786Z"
    },
    "id": "CMXkHmFc7G9m",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size': 128, # Increase this if your GPU can handle it\n",
    "    'lr': 0.1,\n",
    "    'epochs': 30, # 20 epochs is recommended ONLY for the early submission - you will have to train for much longer typically.\n",
    "    'num_classes': 8631, #Dataset contains 8631 classes for classification, reduce this number if you want to train on a subset, but only for train dataset and not on val dataset\n",
    "    'cls_data_dir': \"/kaggle/input/11785-hw-2-p-2-face-verification-spring-2025/HW2p2_S25/cls_data\", #TODO: Provide path of classification directory\n",
    "    'ver_data_dir': \"/kaggle/input/11785-hw-2-p-2-face-verification-spring-2025/HW2p2_S25/ver_data\", #TODO: Provide path of verification directory\n",
    "    'val_pairs_file': \"/kaggle/input/11785-hw-2-p-2-face-verification-spring-2025/HW2p2_S25/val_pairs.txt\", #TODO: Provide path of text file containing val pairs for verification\n",
    "    'test_pairs_file': \"/kaggle/input/11785-hw-2-p-2-face-verification-spring-2025/HW2p2_S25/test_pairs.txt\", #TODO: Provide path of text file containing test pairs for verification\n",
    "    'checkpoint_dir': \"/kaggle/working/\", #TODO: Checkpoint directory\n",
    "    'augument': True\n",
    "    # Include other parameters as needed.\n",
    "}\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEAW65sB8Wlp"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:49:39.754388Z",
     "iopub.status.busy": "2025-02-27T05:49:39.754050Z",
     "iopub.status.idle": "2025-02-27T05:49:39.761096Z",
     "shell.execute_reply": "2025-02-27T05:49:39.760206Z",
     "shell.execute_reply.started": "2025-02-27T05:49:39.754357Z"
    },
    "id": "x-jrgnbQyR2s",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_transforms(image_size: int = 112, augment: bool = True) -> T.Compose:\n",
    "    \"\"\"Create transform pipeline for face recognition.\"\"\"\n",
    "\n",
    "    # Step 1: Basic transformations\n",
    "    transform_list = [\n",
    "        # Resize the image to the desired size (image_size x image_size)\n",
    "        T.Resize((image_size, image_size)),\n",
    "\n",
    "        # Convert PIL Image to tensor\n",
    "        T.ToTensor(),\n",
    "\n",
    "        # Convert image to float32 and scale the pixel values to [0, 1]\n",
    "        T.ToDtype(torch.float32, scale=True),\n",
    "    ]\n",
    "\n",
    "    # Step 2: Data augmentation (optional, based on `augment` argument)\n",
    "    if augment:  # This block will be executed if `augment=True`\n",
    "        # TODO: Add transformations for data augmentation (e.g., random horizontal flip, rotation, etc.)\n",
    "        # Example:\n",
    "        transform_list.extend([\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.RandomRotation(degrees=10),\n",
    "            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2,hue=0.1),\n",
    "            # add random erasing for robustness\n",
    "            T.RandomErasing(p=0.2)\n",
    "        ])\n",
    "\n",
    "    # Step 3: Standard normalization for image recognition tasks\n",
    "    # The Normalize transformation requires mean and std values for each channel (R, G, B).\n",
    "    # Here, we are normalizing the pixel values to have a mean of 0.5 and std of 0.5 for each channel.\n",
    "    transform_list.extend([\n",
    "        T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Standard mean and std for face recognition tasks\n",
    "    ])\n",
    "\n",
    "    # Return the composed transformation pipeline\n",
    "    return T.Compose(transform_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzxNNzGe8ccB"
   },
   "source": [
    "## Classification Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:49:55.497609Z",
     "iopub.status.busy": "2025-02-27T05:49:55.497189Z",
     "iopub.status.idle": "2025-02-27T05:49:55.715647Z",
     "shell.execute_reply": "2025-02-27T05:49:55.714761Z",
     "shell.execute_reply.started": "2025-02-27T05:49:55.497566Z"
    },
    "id": "p6Nn57B-7F-i",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom dataset for loading image-label pairs.\"\"\"\n",
    "    def __init__(self, root, transform, num_classes=config['num_classes']):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (str): Path to the directory containing the images folder.\n",
    "            transform (callable): Transform to be applied to the images.\n",
    "            num_classes (int, optional): Number of classes to keep. If None, keep all classes.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.labels_file = os.path.join(self.root, \"labels.txt\")\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = set()\n",
    "\n",
    "        # Read image-label pairs from the file\n",
    "        with open(self.labels_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        lines = sorted(lines, key=lambda x: int(x.strip().split(' ')[-1]))\n",
    "\n",
    "        # Get all unique labels first\n",
    "        all_labels = sorted(set(int(line.strip().split(' ')[1]) for line in lines))\n",
    "\n",
    "         # Select subset of classes if specified\n",
    "        if num_classes is not None:\n",
    "            selected_classes = set(all_labels[:num_classes])\n",
    "        else:\n",
    "            selected_classes = set(all_labels)\n",
    "\n",
    "        # Store image paths and labels with a progress bar\n",
    "        for line in tqdm(lines, desc=\"Loading dataset\"):\n",
    "            img_path, label = line.strip().split(' ')\n",
    "            label = int(label)\n",
    "\n",
    "            # Only add if label is in selected classes\n",
    "            if label in selected_classes:\n",
    "                self.image_paths.append(os.path.join(self.root, 'images', img_path))\n",
    "                self.labels.append(label)\n",
    "                self.classes.add(label)\n",
    "\n",
    "        assert len(self.image_paths) == len(self.labels), \"Images and labels mismatch!\"\n",
    "\n",
    "        # Convert classes to a sorted list\n",
    "        self.classes = sorted(self.classes)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples.\"\"\"\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (transformed image, label)\n",
    "        \"\"\"\n",
    "        # Load and transform image on-the-fly\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:49:57.420899Z",
     "iopub.status.busy": "2025-02-27T05:49:57.420610Z",
     "iopub.status.idle": "2025-02-27T05:49:57.425258Z",
     "shell.execute_reply": "2025-02-27T05:49:57.424075Z",
     "shell.execute_reply.started": "2025-02-27T05:49:57.420876Z"
    },
    "id": "f3d1tsYE0pb-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train transforms\n",
    "train_transforms = create_transforms(augment=config['augument'])\n",
    "\n",
    "# val transforms\n",
    "val_transforms   = create_transforms(augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:49:58.558396Z",
     "iopub.status.busy": "2025-02-27T05:49:58.558056Z",
     "iopub.status.idle": "2025-02-27T05:50:00.451452Z",
     "shell.execute_reply": "2025-02-27T05:50:00.450677Z",
     "shell.execute_reply.started": "2025-02-27T05:49:58.558366Z"
    },
    "id": "GOJOrQLX02Gh",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Datasets\n",
    "cls_train_dataset = ImageDataset(os.path.join(config['cls_data_dir'],'train'), train_transforms, num_classes=config['num_classes'])#TODO\n",
    "cls_val_dataset   = ImageDataset(os.path.join(config['cls_data_dir'],'dev'), val_transforms, num_classes=config['num_classes'])#TODO\n",
    "cls_test_dataset  = ImageDataset(os.path.join(config['cls_data_dir'],'test'), val_transforms, num_classes=config['num_classes'])#TODO\n",
    "\n",
    "assert cls_train_dataset.classes == cls_val_dataset.classes == cls_test_dataset.classes, \"Class mismatch!\"\n",
    "\n",
    "\n",
    "# Dataloaders\n",
    "cls_train_loader = DataLoader(cls_train_dataset, batch_size=config['batch_size'], shuffle=True,  num_workers=4, pin_memory=True)\n",
    "cls_val_loader   = DataLoader(cls_val_dataset,   batch_size=config['batch_size'], shuffle=False, num_workers=4, pin_memory=True)\n",
    "cls_test_loader  = DataLoader(cls_test_dataset,  batch_size=config['batch_size'], shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPSk8DyK8htk"
   },
   "source": [
    "## Verification Dataset and Datatloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:50:02.050321Z",
     "iopub.status.busy": "2025-02-27T05:50:02.050034Z",
     "iopub.status.idle": "2025-02-27T05:50:02.057811Z",
     "shell.execute_reply": "2025-02-27T05:50:02.056921Z",
     "shell.execute_reply.started": "2025-02-27T05:50:02.050299Z"
    },
    "id": "KBleUieO8lwG",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImagePairDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom dataset for loading and transforming image pairs.\"\"\"\n",
    "    def __init__(self, root, pairs_file, transform):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (str): Path to the directory containing the images.\n",
    "            pairs_file (str): Path to the file containing image pairs and match labels.\n",
    "            transform (callable): Transform to be applied to the images.\n",
    "        \"\"\"\n",
    "        self.root      = root\n",
    "        self.transform = transform\n",
    "\n",
    "        self.matches     = []\n",
    "        self.image1_list = []\n",
    "        self.image2_list = []\n",
    "\n",
    "        # Read and load image pairs and match labels\n",
    "        with open(pairs_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in tqdm(lines, desc=\"Loading image pairs\"):\n",
    "            img_path1, img_path2, match = line.strip().split(' ')\n",
    "            img1 = Image.open(os.path.join(self.root, img_path1)).convert('RGB')\n",
    "            img2 = Image.open(os.path.join(self.root, img_path2)).convert('RGB')\n",
    "\n",
    "            self.image1_list.append(img1)\n",
    "            self.image2_list.append(img2)\n",
    "            self.matches.append(int(match))  # Convert match to integer\n",
    "\n",
    "        assert len(self.image1_list) == len(self.image2_list) == len(self.matches), \"Image pair mismatch\"\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples.\"\"\"\n",
    "        return len(self.image1_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (transformed image1, transformed image2, match label)\n",
    "        \"\"\"\n",
    "        img1 = self.image1_list[idx]\n",
    "        img2 = self.image2_list[idx]\n",
    "        match = self.matches[idx]\n",
    "        return self.transform(img1), self.transform(img2), match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:50:29.167510Z",
     "iopub.status.busy": "2025-02-27T05:50:29.167105Z",
     "iopub.status.idle": "2025-02-27T05:50:29.174555Z",
     "shell.execute_reply": "2025-02-27T05:50:29.173542Z",
     "shell.execute_reply.started": "2025-02-27T05:50:29.167479Z"
    },
    "id": "BxXF96_Ys8os",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TestImagePairDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom dataset for loading and transforming image pairs.\"\"\"\n",
    "    def __init__(self, root, pairs_file, transform):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (str): Path to the directory containing the images.\n",
    "            pairs_file (str): Path to the file containing image pairs and match labels.\n",
    "            transform (callable): Transform to be applied to the images.\n",
    "        \"\"\"\n",
    "        self.root      = root\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image1_list = []\n",
    "        self.image2_list = []\n",
    "\n",
    "        # Read and load image pairs and match labels\n",
    "        with open(pairs_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in tqdm(lines, desc=\"Loading image pairs\"):\n",
    "            img_path1, img_path2 = line.strip().split(' ')\n",
    "            img1 = Image.open(os.path.join(self.root, img_path1)).convert('RGB')\n",
    "            img2 = Image.open(os.path.join(self.root, img_path2)).convert('RGB')\n",
    "\n",
    "            self.image1_list.append(img1)\n",
    "            self.image2_list.append(img2)\n",
    "\n",
    "        assert len(self.image1_list) == len(self.image2_list), \"Image pair mismatch\"\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples.\"\"\"\n",
    "        return len(self.image1_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (transformed image1, transformed image2, match label)\n",
    "        \"\"\"\n",
    "        img1 = self.image1_list[idx]\n",
    "        img2 = self.image2_list[idx]\n",
    "        return self.transform(img1), self.transform(img2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:50:31.214235Z",
     "iopub.status.busy": "2025-02-27T05:50:31.213927Z",
     "iopub.status.idle": "2025-02-27T05:51:19.871543Z",
     "shell.execute_reply": "2025-02-27T05:51:19.870708Z",
     "shell.execute_reply.started": "2025-02-27T05:50:31.214213Z"
    },
    "id": "-ALvOc6L2r3d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Datasets\n",
    "ver_val_dataset  = ImagePairDataset(config['ver_data_dir'], config['val_pairs_file'], val_transforms)#TODO\n",
    "ver_test_dataset = TestImagePairDataset(config['ver_data_dir'], config['test_pairs_file'], val_transforms)#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:51:28.029576Z",
     "iopub.status.busy": "2025-02-27T05:51:28.029131Z",
     "iopub.status.idle": "2025-02-27T05:51:28.034283Z",
     "shell.execute_reply": "2025-02-27T05:51:28.033138Z",
     "shell.execute_reply.started": "2025-02-27T05:51:28.029534Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "ver_val_loader   = DataLoader(ver_val_dataset,  batch_size=config['batch_size'], shuffle=False, num_workers=4, pin_memory=True)\n",
    "ver_test_loader  = DataLoader(ver_test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2j24TXNo9P97"
   },
   "source": [
    "## Create Dataloaders for Image Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "436KzM6u-3A2"
   },
   "source": [
    "# EDA and Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:51:29.217149Z",
     "iopub.status.busy": "2025-02-27T05:51:29.216859Z",
     "iopub.status.idle": "2025-02-27T05:51:29.288357Z",
     "shell.execute_reply": "2025-02-27T05:51:29.287528Z",
     "shell.execute_reply.started": "2025-02-27T05:51:29.217127Z"
    },
    "id": "AhnoHopx-0RB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Double-check your dataset/dataloaders work as expected\n",
    "\n",
    "print(\"Number of classes    : \", len(cls_train_dataset.classes))\n",
    "print(\"No. of train images  : \", cls_train_dataset.__len__())\n",
    "print(\"Shape of image       : \", cls_train_dataset[0][0].shape)\n",
    "print(\"Batch size           : \", config['batch_size'])\n",
    "print(\"Train batches        : \", cls_train_loader.__len__())\n",
    "print(\"Val batches          : \", cls_val_loader.__len__())\n",
    "\n",
    "# Feel free to print more things if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niSJ49lzpHgW"
   },
   "source": [
    "### Classification Dataset Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:51:32.765377Z",
     "iopub.status.busy": "2025-02-27T05:51:32.765033Z",
     "iopub.status.idle": "2025-02-27T05:51:36.521887Z",
     "shell.execute_reply": "2025-02-27T05:51:36.515695Z",
     "shell.execute_reply.started": "2025-02-27T05:51:32.765351Z"
    },
    "id": "fK3t65VU5Qlz",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def show_cls_dataset_samples(train_loader, val_loader, test_loader, samples_per_set=8, figsize=(10, 6)):\n",
    "    \"\"\"\n",
    "    Display samples from train, validation, and test datasets side by side\n",
    "\n",
    "    Args:\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        test_loader: Test data loader\n",
    "        samples_per_set: Number of samples to show from each dataset\n",
    "        figsize: Figure size (width, height)\n",
    "    \"\"\"\n",
    "    def denormalize(x):\n",
    "        \"\"\"Denormalize images from [-1, 1] to [0, 1]\"\"\"\n",
    "        return x * 0.5 + 0.5\n",
    "\n",
    "    def get_samples(loader, n):\n",
    "        \"\"\"Get n samples from a dataloader\"\"\"\n",
    "        batch = next(iter(loader))\n",
    "        return batch[0][:n], batch[1][:n]\n",
    "\n",
    "    # Get samples from each dataset\n",
    "    train_imgs, train_labels = get_samples(train_loader, samples_per_set)\n",
    "    val_imgs, val_labels = get_samples(val_loader, samples_per_set)\n",
    "    test_imgs, test_labels = get_samples(test_loader, samples_per_set)\n",
    "\n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(3, 1, figsize=figsize)\n",
    "\n",
    "    # Plot each dataset\n",
    "    for idx, (imgs, labels, title) in enumerate([\n",
    "        (train_imgs, train_labels, 'Training Samples'),\n",
    "        (val_imgs, val_labels, 'Validation Samples'),\n",
    "        (test_imgs, test_labels, 'Test Samples')\n",
    "    ]):\n",
    "\n",
    "        # Create grid of images\n",
    "        grid = make_grid(denormalize(imgs), nrow=8, padding=2)\n",
    "\n",
    "        # Display grid\n",
    "        axes[idx].imshow(grid.permute(1, 2, 0).cpu())\n",
    "        axes[idx].axis('off')\n",
    "        axes[idx].set_title(title, fontsize=10)\n",
    "\n",
    "        # Add class labels below images (with smaller font)\n",
    "        grid_width = grid.shape[2]\n",
    "        imgs_per_row = min(8, samples_per_set)\n",
    "        img_width = grid_width // imgs_per_row\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            col = i % imgs_per_row  # Calculate column position\n",
    "            if label<len(train_loader.dataset.classes):\n",
    "              class_name = train_loader.dataset.classes[label]\n",
    "            else:\n",
    "              class_name = f\"Class {label} (Unknown)\"\n",
    "            axes[idx].text(col * img_width + img_width/2,\n",
    "                         grid.shape[1] + 5,\n",
    "                         class_name,\n",
    "                         ha='center',\n",
    "                         va='top',\n",
    "                         fontsize=6,\n",
    "                         rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_cls_dataset_samples(cls_train_loader, cls_val_loader, cls_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZPVGXOu59Wh"
   },
   "source": [
    "###Ver Dataset Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:51:43.124074Z",
     "iopub.status.busy": "2025-02-27T05:51:43.123743Z",
     "iopub.status.idle": "2025-02-27T05:51:44.173881Z",
     "shell.execute_reply": "2025-02-27T05:51:44.172808Z",
     "shell.execute_reply.started": "2025-02-27T05:51:43.124047Z"
    },
    "id": "6pBN7Z9K5iAM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def show_ver_dataset_samples(val_loader, samples_per_set=4, figsize=(12, 8)):\n",
    "    \"\"\"\n",
    "    Display verification pairs from the validation dataset\n",
    "\n",
    "    Args:\n",
    "        val_loader: Validation data loader\n",
    "        samples_per_set: Number of pairs to show from the dataset\n",
    "        figsize: Figure size (width, height)\n",
    "    \"\"\"\n",
    "    def denormalize(x):\n",
    "        \"\"\"Denormalize images from [-1, 1] to [0, 1]\"\"\"\n",
    "        return x * 0.5 + 0.5\n",
    "\n",
    "    def get_samples(loader, n):\n",
    "        \"\"\"Get n samples from a dataloader\"\"\"\n",
    "        batch = next(iter(loader))\n",
    "        return batch[0][:n], batch[1][:n], batch[2][:n]\n",
    "\n",
    "    # Get samples from the validation dataset\n",
    "    val_imgs1, val_imgs2, val_labels = get_samples(val_loader, samples_per_set)\n",
    "\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    # Create grids for both images in each pair\n",
    "    grid1 = make_grid(denormalize(val_imgs1), nrow=samples_per_set, padding=2)\n",
    "    grid2 = make_grid(denormalize(val_imgs2), nrow=samples_per_set, padding=2)\n",
    "\n",
    "    # Combine the grids vertically\n",
    "    combined_grid = torch.cat([grid1, grid2], dim=1)\n",
    "\n",
    "    # Display the combined grid\n",
    "    ax.imshow(combined_grid.permute(1, 2, 0).cpu())\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Validation Pairs', fontsize=10)\n",
    "\n",
    "    # Determine dimensions for placing the labels\n",
    "    grid_width = grid1.shape[2]\n",
    "    img_width = grid_width // samples_per_set\n",
    "\n",
    "    # Add match/non-match labels for each pair\n",
    "    for i, label in enumerate(val_labels):\n",
    "        match_text = \"✓ Match\" if label == 1 else \"✗ Non-match\"\n",
    "        color = 'green' if label == 1 else 'red'\n",
    "\n",
    "        # Define a background box for the label\n",
    "        bbox_props = dict(\n",
    "            boxstyle=\"round,pad=0.3\",\n",
    "            fc=\"white\",\n",
    "            ec=color,\n",
    "            alpha=0.8\n",
    "        )\n",
    "\n",
    "        ax.text(i * img_width + img_width / 2,\n",
    "                combined_grid.shape[1] + 15,  # Position below the images\n",
    "                match_text,\n",
    "                ha='center',\n",
    "                va='top',\n",
    "                fontsize=8,\n",
    "                color=color,\n",
    "                bbox=bbox_props)\n",
    "\n",
    "    plt.suptitle(\"Verification Pairs (Top: Image 1, Bottom: Image 2)\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.05)\n",
    "    plt.show()\n",
    "\n",
    "show_ver_dataset_samples(ver_val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3TUocDw_JU_"
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:51:51.660122Z",
     "iopub.status.busy": "2025-02-27T05:51:51.659766Z",
     "iopub.status.idle": "2025-02-27T05:51:52.509937Z",
     "shell.execute_reply": "2025-02-27T05:51:52.508972Z",
     "shell.execute_reply.started": "2025-02-27T05:51:51.660096Z"
    },
    "id": "4LLX2Rki_LzA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO: Fill out the model definition below\n",
    "C=[3, 64,128,256,512,1024] # CHANNELs\n",
    "\n",
    "# Define ResidualBlock for ResNet architecture\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        # Main branch: two conv layers\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(out_channels)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Shortcut branch: if shape changes, adjust with 1x1 conv\n",
    "        self.shortcut = torch.nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                torch.nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # Compute the residual\n",
    "        residual = self.relu(self.bn1(self.conv1(x)))\n",
    "        residual = self.bn2(self.conv2(residual))\n",
    "        \n",
    "        # Shortcut: either identity or projection\n",
    "        identity = self.shortcut(x)\n",
    "        \n",
    "        # Element-wise addition followed by activation\n",
    "        out = self.relu(residual + identity)\n",
    "        return out\n",
    "\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial stem layer (similar to original layer 1)\n",
    "        self.stem = torch.nn.Sequential(\n",
    "            # layer 1 - modified to be first part of ResNet\n",
    "            torch.nn.Conv2d(C[0], C[1], 7, stride=2, padding=3),\n",
    "            torch.nn.BatchNorm2d(C[1]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        # Residual layers (replacing original layers 2-5)\n",
    "        self.layer1 = self._make_layer(C[1], C[1], num_blocks=2, stride=1)\n",
    "        self.layer2 = self._make_layer(C[1], C[2], num_blocks=2, stride=2)\n",
    "        self.layer3 = self._make_layer(C[2], C[3], num_blocks=2, stride=2)\n",
    "        self.layer4 = self._make_layer(C[3], C[4], num_blocks=2, stride=2)\n",
    "        self.layer5 = self._make_layer(C[4], C[5], num_blocks=2, stride=1)\n",
    "        \n",
    "        # Adaptive average pooling to get 1x1 spatial dimensions\n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Flatten the 1x1 spatial dimensions\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        \n",
    "        # Classification layer: Linear layer from C[5] to num_classes\n",
    "        # fully connected (Linear) layer to map the features to the desired number of output classes.\n",
    "        self.cls_layer = torch.nn.Linear(C[5], num_classes)\n",
    "    \n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        # First block (handles downsampling if needed)\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
    "        \n",
    "        # Remaining blocks (keep size the same)\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels, stride=1))\n",
    "            \n",
    "        return torch.nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Process through stem and residual layers\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        \n",
    "        # Global pooling and flatten\n",
    "        x = self.avgpool(x)\n",
    "        feats = self.flatten(x)\n",
    "        \n",
    "        # Classification layer\n",
    "        out = self.cls_layer(feats)\n",
    "        \n",
    "        return {\"feats\": feats, \"out\": out}\n",
    "\n",
    "model = Network(num_classes=config['num_classes']).to(DEVICE)\n",
    "summary(model, (3, 112, 112))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:51:57.518311Z",
     "iopub.status.busy": "2025-02-27T05:51:57.517998Z",
     "iopub.status.idle": "2025-02-27T05:51:57.527246Z",
     "shell.execute_reply": "2025-02-27T05:51:57.526102Z",
     "shell.execute_reply.started": "2025-02-27T05:51:57.518287Z"
    },
    "id": "pDP--pND_3Vy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------- #\n",
    "\n",
    "# Defining Loss function\n",
    "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# --------------------------------------------------- #\n",
    "\n",
    "# Defining Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# --------------------------------------------------- #\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
    "\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=1e-3, \n",
    "    total_steps=config['epochs'] * len(cls_train_loader),\n",
    "    pct_start=0.3,  #to warm-up\n",
    "    div_factor=25,  #starting lr\n",
    "    final_div_factor=1000, #final lr\n",
    "    anneal_strategy='cos'\n",
    ")\n",
    "\n",
    "# --------------------------------------------------- #\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-d5ZDQfpw7gR"
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:52:03.145470Z",
     "iopub.status.busy": "2025-02-27T05:52:03.144983Z",
     "iopub.status.idle": "2025-02-27T05:52:03.152528Z",
     "shell.execute_reply": "2025-02-27T05:52:03.151414Z",
     "shell.execute_reply.started": "2025-02-27T05:52:03.145407Z"
    },
    "id": "7Ecg0J2sw9jJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:52:04.289636Z",
     "iopub.status.busy": "2025-02-27T05:52:04.289115Z",
     "iopub.status.idle": "2025-02-27T05:52:04.296886Z",
     "shell.execute_reply": "2025-02-27T05:52:04.295674Z",
     "shell.execute_reply.started": "2025-02-27T05:52:04.289579Z"
    },
    "id": "TqVw0ab0xBKT",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    maxk = min(max(topk), output.size()[1])\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
    "    return [correct[:min(k, maxk)].reshape(-1).float().sum(0) * 100. / batch_size for k in topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:52:05.514611Z",
     "iopub.status.busy": "2025-02-27T05:52:05.514133Z",
     "iopub.status.idle": "2025-02-27T05:52:05.523253Z",
     "shell.execute_reply": "2025-02-27T05:52:05.522286Z",
     "shell.execute_reply.started": "2025-02-27T05:52:05.514558Z"
    },
    "id": "uNCQjz2RxD5S",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_ver_metrics(labels, scores, FPRs):\n",
    "    # eer and auc\n",
    "    fpr, tpr, _ = mt.roc_curve(labels, scores, pos_label=1)\n",
    "    roc_curve = interp1d(fpr, tpr)\n",
    "    EER = 100. * brentq(lambda x : 1. - x - roc_curve(x), 0., 1.)\n",
    "    AUC = 100. * mt.auc(fpr, tpr)\n",
    "\n",
    "    # get acc\n",
    "    tnr = 1. - fpr\n",
    "    pos_num = labels.count(1)\n",
    "    neg_num = labels.count(0)\n",
    "    ACC = 100. * max(tpr * pos_num + tnr * neg_num) / len(labels)\n",
    "\n",
    "    # TPR @ FPR\n",
    "    if isinstance(FPRs, list):\n",
    "        TPRs = [\n",
    "            ('TPR@FPR={}'.format(FPR), 100. * roc_curve(float(FPR)))\n",
    "            for FPR in FPRs\n",
    "        ]\n",
    "    else:\n",
    "        TPRs = []\n",
    "\n",
    "    return {\n",
    "        'ACC': ACC,\n",
    "        'EER': EER,\n",
    "        'AUC': AUC,\n",
    "        'TPRs': TPRs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juUbZnP0AEUi"
   },
   "source": [
    "# Train and Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:52:07.182159Z",
     "iopub.status.busy": "2025-02-27T05:52:07.181836Z",
     "iopub.status.idle": "2025-02-27T05:52:07.190996Z",
     "shell.execute_reply": "2025-02-27T05:52:07.189915Z",
     "shell.execute_reply.started": "2025-02-27T05:52:07.182134Z"
    },
    "id": "IMnxvQT-AHsu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, lr_scheduler, scaler, device, config):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # metric meters\n",
    "    loss_m = AverageMeter()\n",
    "    acc_m = AverageMeter()\n",
    "\n",
    "    # Progress Bar\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)\n",
    "\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "\n",
    "        optimizer.zero_grad() # Zero gradients\n",
    "\n",
    "        # send to cuda\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        if isinstance(labels, (tuple, list)):\n",
    "            targets1, targets2, lam = labels\n",
    "            labels = (targets1.to(device), targets2.to(device), lam)\n",
    "        else:\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():  # This implements mixed precision. Thats it!\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Use the type of output depending on the loss function you want to use\n",
    "            loss = criterion(outputs['out'], labels)\n",
    "\n",
    "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
    "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
    "        scaler.update()\n",
    "        # metrics\n",
    "        loss_m.update(loss.item())\n",
    "        if 'feats' in outputs:\n",
    "            acc = accuracy(outputs['out'], labels)[0].item()\n",
    "        else:\n",
    "            acc = 0.0\n",
    "        acc_m.update(acc)\n",
    "\n",
    "        # tqdm lets you add some details so you can monitor training as you train.\n",
    "        batch_bar.set_postfix(\n",
    "            # acc         = \"{:.04f}%\".format(100*accuracy),\n",
    "            acc=\"{:.04f}% ({:.04f})\".format(acc, acc_m.avg),\n",
    "            loss        = \"{:.04f} ({:.04f})\".format(loss.item(), loss_m.avg),\n",
    "            lr          = \"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "\n",
    "        batch_bar.update() # Update tqdm bar\n",
    "    \n",
    "    # You may want to call some schedulers inside the train function. What are these?\n",
    "    if lr_scheduler is not None and not isinstance(lr_scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    batch_bar.close()\n",
    "\n",
    "    return acc_m.avg, loss_m.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:52:09.122182Z",
     "iopub.status.busy": "2025-02-27T05:52:09.121797Z",
     "iopub.status.idle": "2025-02-27T05:52:09.128851Z",
     "shell.execute_reply": "2025-02-27T05:52:09.127886Z",
     "shell.execute_reply.started": "2025-02-27T05:52:09.122138Z"
    },
    "id": "5qkdH295wNUX",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_epoch_cls(model, dataloader, device, config):\n",
    "\n",
    "    model.eval()\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val Cls.', ncols=5)\n",
    "\n",
    "    # metric meters\n",
    "    loss_m = AverageMeter()\n",
    "    acc_m = AverageMeter()\n",
    "\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "\n",
    "        # Move images to device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Get model outputs\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs['out'], labels)\n",
    "\n",
    "        # metrics\n",
    "        acc = accuracy(outputs['out'], labels)[0].item()\n",
    "        loss_m.update(loss.item())\n",
    "        acc_m.update(acc)\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            acc         = \"{:.04f}% ({:.04f})\".format(acc, acc_m.avg),\n",
    "            loss        = \"{:.04f} ({:.04f})\".format(loss.item(), loss_m.avg))\n",
    "\n",
    "        batch_bar.update()\n",
    "\n",
    "    batch_bar.close()\n",
    "    return acc_m.avg, loss_m.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:52:11.725347Z",
     "iopub.status.busy": "2025-02-27T05:52:11.725034Z",
     "iopub.status.idle": "2025-02-27T05:52:11.990179Z",
     "shell.execute_reply": "2025-02-27T05:52:11.989330Z",
     "shell.execute_reply.started": "2025-02-27T05:52:11.725310Z"
    },
    "id": "-Yan5vLDyj-3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gc.collect() # These commands help you when you face CUDA OOM error\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0q1gRMAsyknz"
   },
   "source": [
    "# Verification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:52:13.261866Z",
     "iopub.status.busy": "2025-02-27T05:52:13.261523Z",
     "iopub.status.idle": "2025-02-27T05:52:13.269091Z",
     "shell.execute_reply": "2025-02-27T05:52:13.267978Z",
     "shell.execute_reply.started": "2025-02-27T05:52:13.261841Z"
    },
    "id": "SSGeDCi-wa1W",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def valid_epoch_ver(model, pair_data_loader, device, config):\n",
    "\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    match_labels = []\n",
    "    batch_bar = tqdm(total=len(pair_data_loader), dynamic_ncols=True, position=0, leave=False, desc='Val Veri.')\n",
    "    for i, (images1, images2, labels) in enumerate(pair_data_loader):\n",
    "\n",
    "        # match_labels = match_labels.to(device)\n",
    "        images = torch.cat([images1, images2], dim=0).to(device)\n",
    "        # Get model outputs\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(images)\n",
    "\n",
    "        feats = F.normalize(outputs['feats'], dim=1)\n",
    "        feats1, feats2 = feats.chunk(2)\n",
    "        similarity = F.cosine_similarity(feats1, feats2)\n",
    "        scores.append(similarity.cpu().numpy())\n",
    "        match_labels.append(labels.cpu().numpy())\n",
    "        batch_bar.update()\n",
    "\n",
    "    scores = np.concatenate(scores)\n",
    "    match_labels = np.concatenate(match_labels)\n",
    "\n",
    "    FPRs=['1e-4', '5e-4', '1e-3', '5e-3', '5e-2']\n",
    "    metric_dict = get_ver_metrics(match_labels.tolist(), scores.tolist(), FPRs)\n",
    "    print(metric_dict)\n",
    "\n",
    "    return metric_dict['ACC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piblCbe5yotj"
   },
   "source": [
    "# WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:52:15.365145Z",
     "iopub.status.busy": "2025-02-27T05:52:15.364779Z",
     "iopub.status.idle": "2025-02-27T05:52:21.826356Z",
     "shell.execute_reply": "2025-02-27T05:52:21.825650Z",
     "shell.execute_reply.started": "2025-02-27T05:52:15.365113Z"
    },
    "id": "HTIkCXBQyoM0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.login(key=\"06a27b50be8dd39b3865d8e90bb7c3248eafc168\") # API Key is in your wandb account, under settings (wandb.ai/settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:52:28.034330Z",
     "iopub.status.busy": "2025-02-27T05:52:28.033758Z",
     "iopub.status.idle": "2025-02-27T05:52:34.469416Z",
     "shell.execute_reply": "2025-02-27T05:52:34.468728Z",
     "shell.execute_reply.started": "2025-02-27T05:52:28.034300Z"
    },
    "id": "GLNNqwV4ysNP",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create your wandb run\n",
    "run = wandb.init(\n",
    "    name = \"sub-2\", ## Wandb creates random run names if you skip this field\n",
    "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
    "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project = \"hw2p2-ablations\", ### Project should be created in your wandb account\n",
    "    config = config ### Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0RrtpFKzH3k"
   },
   "source": [
    "# Checkpointing and Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:52:36.389023Z",
     "iopub.status.busy": "2025-02-27T05:52:36.388673Z",
     "iopub.status.idle": "2025-02-27T05:52:36.394255Z",
     "shell.execute_reply": "2025-02-27T05:52:36.393385Z",
     "shell.execute_reply.started": "2025-02-27T05:52:36.388993Z"
    },
    "id": "O1gbAkMtlWHk",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_dir = config['checkpoint_dir']\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:52:37.849513Z",
     "iopub.status.busy": "2025-02-27T05:52:37.849145Z",
     "iopub.status.idle": "2025-02-27T05:52:37.856460Z",
     "shell.execute_reply": "2025-02-27T05:52:37.855112Z",
     "shell.execute_reply.started": "2025-02-27T05:52:37.849482Z"
    },
    "id": "dDFmC8hpzLOq",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, metrics, epoch, path):\n",
    "    torch.save(\n",
    "        {'model_state_dict'         : model.state_dict(),\n",
    "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
    "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
    "         'metric'                   : metrics,\n",
    "         'epoch'                    : epoch},\n",
    "         path)\n",
    "\n",
    "\n",
    "def load_model(model, optimizer=None, scheduler=None, path='./checkpoint.pth'):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    else:\n",
    "        optimizer = None\n",
    "    if scheduler is not None:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    else:\n",
    "        scheduler = None\n",
    "    epoch = checkpoint['epoch']\n",
    "    metrics = checkpoint['metric']\n",
    "    return model, optimizer, scheduler, epoch, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpFT7iriy5bi"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T05:52:39.630330Z",
     "iopub.status.busy": "2025-02-27T05:52:39.629989Z",
     "iopub.status.idle": "2025-02-27T06:09:56.207854Z",
     "shell.execute_reply": "2025-02-27T06:09:56.206685Z",
     "shell.execute_reply.started": "2025-02-27T05:52:39.630297Z"
    },
    "id": "59FcCeJfy3Zm",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "e = 0\n",
    "best_valid_cls_acc = 0.0\n",
    "eval_cls = True\n",
    "best_valid_ret_acc = 0.0\n",
    "for epoch in range(e, config['epochs']):\n",
    "        # epoch\n",
    "        print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
    "\n",
    "        # train\n",
    "        train_cls_acc, train_loss = train_epoch(model, cls_train_loader, optimizer, scheduler, scaler, DEVICE, config)\n",
    "        curr_lr = float(optimizer.param_groups[0]['lr'])\n",
    "        print(\"\\nEpoch {}/{}: \\nTrain Cls. Acc {:.04f}%\\t Train Cls. Loss {:.04f}\\t Learning Rate {:.04f}\".format(epoch + 1, config['epochs'], train_cls_acc, train_loss, curr_lr))\n",
    "        metrics = {\n",
    "            'train_cls_acc': train_cls_acc,\n",
    "            'train_loss': train_loss,\n",
    "        }\n",
    "\n",
    "        # classification validation\n",
    "        if eval_cls:\n",
    "            valid_cls_acc, valid_loss = valid_epoch_cls(model, cls_val_loader, DEVICE, config)\n",
    "            print(\"Val Cls. Acc {:.04f}%\\t Val Cls. Loss {:.04f}\".format(valid_cls_acc, valid_loss))\n",
    "            metrics.update({\n",
    "                'valid_cls_acc': valid_cls_acc,\n",
    "                'valid_loss': valid_loss,\n",
    "            })\n",
    "            \n",
    "            # Update ReduceLROnPlateau scheduler with validation loss\n",
    "            if scheduler is not None and isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(valid_loss)\n",
    "        \n",
    "\n",
    "        # retrieval validation\n",
    "        valid_ret_acc = valid_epoch_ver(model, ver_val_loader, DEVICE, config)\n",
    "        print(\"Val Ret. Acc {:.04f}%\".format(valid_ret_acc))\n",
    "        metrics.update({\n",
    "            'valid_ret_acc': valid_ret_acc\n",
    "        })\n",
    "\n",
    "        # save model\n",
    "        save_model(model, optimizer, scheduler, metrics, epoch, os.path.join(config['checkpoint_dir'], 'last.pth'))\n",
    "        print(\"Saved epoch model\")\n",
    "\n",
    "        # save best model\n",
    "        if eval_cls:\n",
    "            if valid_cls_acc >= best_valid_cls_acc:\n",
    "                best_valid_cls_acc = valid_cls_acc\n",
    "                save_model(model, optimizer, scheduler, metrics, epoch, os.path.join(config['checkpoint_dir'], 'best_cls.pth'))\n",
    "                wandb.save(os.path.join(config['checkpoint_dir'], 'best_cls.pth'))\n",
    "                print(\"Saved best classification model\")\n",
    "\n",
    "        if valid_ret_acc >= best_valid_ret_acc:\n",
    "            best_valid_ret_acc = valid_ret_acc\n",
    "            save_model(model, optimizer, scheduler, metrics, epoch, os.path.join(config['checkpoint_dir'], 'best_ret.pth'))\n",
    "            wandb.save(os.path.join(config['checkpoint_dir'], 'best_ret.pth'))\n",
    "            print(\"Saved best retrieval model\")\n",
    "\n",
    "        # log to tracker\n",
    "        if run is not None:\n",
    "            run.log(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXLTfQjv0cCb"
   },
   "source": [
    "# Testing and Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T06:19:42.574307Z",
     "iopub.status.busy": "2025-02-27T06:19:42.573937Z",
     "iopub.status.idle": "2025-02-27T06:19:42.581918Z",
     "shell.execute_reply": "2025-02-27T06:19:42.580981Z",
     "shell.execute_reply.started": "2025-02-27T06:19:42.574275Z"
    },
    "id": "XUAa3m2h0eCD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_epoch_ver(model, pair_data_loader, config):\n",
    "\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    batch_bar = tqdm(total=len(pair_data_loader), dynamic_ncols=True, position=0, leave=False, desc='Val Veri.')\n",
    "    for i, (images1, images2) in enumerate(pair_data_loader):\n",
    "\n",
    "        images = torch.cat([images1, images2], dim=0).to(DEVICE)\n",
    "        # Get model outputs\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(images)\n",
    "\n",
    "        feats = F.normalize(outputs['feats'], dim=1)\n",
    "        feats1, feats2 = feats.chunk(2)\n",
    "        similarity = F.cosine_similarity(feats1, feats2)\n",
    "        scores.extend(similarity.cpu().numpy().tolist())\n",
    "        batch_bar.update()\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T06:20:10.331175Z",
     "iopub.status.busy": "2025-02-27T06:20:10.330818Z",
     "iopub.status.idle": "2025-02-27T06:20:15.168886Z",
     "shell.execute_reply": "2025-02-27T06:20:15.166434Z",
     "shell.execute_reply.started": "2025-02-27T06:20:10.331151Z"
    },
    "id": "sZnuyzfD5xdS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scores = test_epoch_ver(model, ver_test_loader, config)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T06:20:22.295100Z",
     "iopub.status.busy": "2025-02-27T06:20:22.294766Z",
     "iopub.status.idle": "2025-02-27T06:20:22.309359Z",
     "shell.execute_reply": "2025-02-27T06:20:22.308655Z",
     "shell.execute_reply.started": "2025-02-27T06:20:22.295072Z"
    },
    "id": "fOLRdaKZ50RQ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"verification_early_submission.csv\", \"w+\") as f:\n",
    "    f.write(\"ID,Label\\n\")\n",
    "    for i in range(len(scores)):\n",
    "        f.write(\"{},{}\\n\".format(i, scores[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T06:25:29.413571Z",
     "iopub.status.busy": "2025-02-27T06:25:29.413101Z",
     "iopub.status.idle": "2025-02-27T06:25:29.420479Z",
     "shell.execute_reply": "2025-02-27T06:25:29.419578Z",
     "shell.execute_reply.started": "2025-02-27T06:25:29.413531Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Create kaggle.json file\n",
    "os.makedirs('/root/.config/kaggle', exist_ok=True)\n",
    "with open('/root/.config/kaggle/kaggle.json', 'w') as f:\n",
    "    json.dump({\n",
    "        \"username\": \"hy8888\",\n",
    "        \"key\": \"8b17a8d9018a6f3b20b62c419e3fe755\"\n",
    "    }, f)\n",
    "\n",
    "# Set permissions\n",
    "os.chmod('/root/.config/kaggle/kaggle.json', 0o600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T06:26:43.453451Z",
     "iopub.status.busy": "2025-02-27T06:26:43.453087Z",
     "iopub.status.idle": "2025-02-27T06:26:46.022979Z",
     "shell.execute_reply": "2025-02-27T06:26:46.022021Z",
     "shell.execute_reply.started": "2025-02-27T06:26:43.453409Z"
    },
    "id": "v3_8VUTQ52zT",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Submit to kaggle competition using kaggle API (Uncomment below to use)\n",
    "!kaggle competitions submit -c 11785-hw-2-p-2-face-verification-spring-2025 -f /kaggle/working/verification_early_submission.csv -m \"Submission-HW2P2\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11055270,
     "sourceId": 91656,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
